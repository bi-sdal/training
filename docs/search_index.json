[
["index.html", "Training Manual for SDAL Welcome", " Training Manual for SDAL Welcome The purpose of this website is to consolodate all of the ‘basic’ trainings and knowledge in the lab to be used as a reference and trainining handbook used in our summer Data Science for the Public Good Program. "],
["the-social-and-decision-analytics-laboratory.html", "The Social and Decision Analytics Laboratory", " The Social and Decision Analytics Laboratory The Social and Decision Analytics Laboratory brings together statisticians and social and behavioral scientists to embrace today’s data revolution, developing evidence-based research and quantitative methods to inform policy decision-making. https://www.bi.vt.edu/sdal/about Overview: Our Mission &amp; Methods As a leading laboratory in the Biocomplexity Institute of Virginia Tech, the Social and Decision Analytics Laboratory is modeling the social condition of metropolitan areas, integrating novel sources of data to examine health and wellness, and uncovering key factors that drive industrial innovation. Social and decision analytics have the power to fundamentally change our understanding of the world around us. Our research informs effective policy-making under uncertainty, combining expertise in statistics and the social sciences to transform all data into actionable knowledge. Leveraging analytics to solve real-world problems requires a broad range of expertise. The SDAL team includes thought leaders in a wide variety of fields: statistics, social, behavioral and economic sciences, political science, psychology, simulation, computer science, data analytics, information technology policy, and infrastructural resilience. SDAL is driven to make data analytics an accessible and effective part of the policy-making process. SDAL’s newest partnerships and recent discoveries are regularly profiled on our news page, while public lectures are available in our presentations archive. Our location at the Virginia Tech Research Center in Arlington, Virginia allows us to partner with local, state, and federal governments, non-profit organizations, and industry throughout the D.C. metro area and beyond. Innovation: Our Research Focus Areas Every global challenge we face today is rooted in information. Advances in big data analysis have the potential to revolutionize the ways we address long-standing social problems such as unequal access to services and affordable housing. To address these and other pressing issues, the Social and Decision Analytics Laboratory is pioneering three major domains of research. The Science of “All” Data aims to integrate disparate sources of data—surveys and experiments; local, state, and federal administrative government records; social media; mobile technology, and geographic information—to deliver a comprehensive understanding of social problems. We are experts in drawing actionable information out of messy data sets of all sizes. Information Diffusion Analytics charts how knowledge and social attitudes spread through information networks using a combination of network science, cognitive modeling, crowd-sourcing, and machine learning. Our research predicts how information reaches people and affects their outlook. Community Learning Data-Driven Discovery (CLD3) allows organizations to identify individually maintained data resources that serve their mutual needs. Information that would have been siloed within a single department is “liberated,” freed for use throughout the community. Our research focuses on how to discovery, liberate, and repurpose these data flows to achieve data-informed policy development. Our unique CLD3 approach is supported by expertise in five sub-domains of analytics research: Metropolitan Analytics applies data on infrastructure, environment, and people to understand how urban areas can support the well-being of their expanding populations. Our research guides the growth of resilient cities. Education and Labor Force Analytics seeks to provide a clearer picture of the factors that help students and employees thrive by understanding learning processes, social interactions, and learning environments. Our work examines the impact of education on labor force outcomes. Health and Well Being Analytics leverages public and private sector medical data to help communities deliver healthcare services where they are most needed. SDAL studies help healthcare resources do more good. Emergency Management Analytics creates deep characterizations of situational awareness at all levels through accessing and integrating comprehensive information of conditions that increase risks and stress to the emergency responders and the citizens they serve. Our research guides the development of equitable policies and practices to improve the safety and security of our communities. Industrial Innovation Analytics utilizes data from partners in the private sector to identify factors that reduce costs and improve efficiency. We enable the private sector to move beyond business analytics to develop information integration technologies. Collaboration: Our Research Partnerships Since its founding in 2013, SDAL has developed its world-class statistical and data science capabilities in support of the Biocomplexity Institute’s overarching mission to predict, explain, and visualize the behavior of massively interacting systems. Backed by the Biocomplexity Institute’s diverse research programs and unique, high-performance computing infrastructure, SDAL has established long-term partnerships with a variety of organizations including Arlington County, Procter &amp; Gamble, the Robert Wood Johnson Foundation, the United States Census Bureau, and the Department of Housing and Urban Development. The Social and Decision Analytics Laboratory welcomes new collaboration opportunities. To inquire, please contact our director, Sallie Keller. "],
["data-science-for-the-public-good-program.html", "Data Science for the Public Good Program", " Data Science for the Public Good Program The Data Science for the Public Good program (DSPG) engages young scholars in finding solutions to some of the most pressing social issues of our time. DSPG fellows conduct research at the intersection of statistics, computation, and the social sciences to determine how information generated within every community can be leveraged to improve quality of life. Problem Our communities are engaged in a tremendous struggle to manage conflicting forces that threaten their ability to evolve and thrive. Government officials and policymakers must continuously develop their capacity to provide health, safety, security, employment, and leisure to their constituents. Sustaining this level of growth is becoming increasingly difficult in a climate of diminishing resources, mounting inequality, rapid technological change, and expanding global networks. Data-driven research provides a rich, mutually rewarding opportunity to leverage community knowledge and public information resources to affect positive social change. The Data Science for the Public Good program was established to connect aspiring data science scholars to communities which can benefit from their expertise. Students selected through a competitive application process are engaged in a series of hands-on learning experiences while policymakers and government leaders receive data analysis support to inform difficult decisions related to healthcare, education, and social justice. The story of each community—its problems, needs, and aspirations—is contained within its data. The DSPG program’s overarching objective is to equip new generations of scientists with the skills they need to bring this story to light for leaders in local government. Methods The Data Science for the Public Good program teaches student fellows how to sift through vast amounts of information related to public safety, employment, and the provision of services to discover how communities can become more efficient and sustainable. Through the lenses of statistics, social science, and data science research, DSPG students will learn to integrate all available data resources in order to: Identify pressing issues through direct engagement with government and community leaders. Develop mechanisms to assist decision-makers in framing their large-scale policy questions and identifying data sources which can be leveraged to address these issues at the local, state, and federal levels. Create a two-way data pipeline to give local leaders a direct link to cutting-edge scientific analyses and researchers easier access to local, state, and federal data flows. DSPG fellows are also given opportunities to diversify their expertise and form lasting professional connections by taking part in the Social and Decision Analytics Laboratory’s data-driven research projects. Our research teams are: Horizontally integrated, combining disciplines including statistics, data science, and the social and behavioral sciences to address complex issues. Vertically integrated, allowing students to collaborate with project stakeholders at all levels including undergraduates, graduates, postdoctoral associates, research faculty, and local, state, and federal agency leadership. These unique fellowship experiences are made possible through the support of several research organizations dedicated to serving the public good: Virginia Tech’s Global Forum for Urban and Regional Resilience (GFURR); American Statistical Association’s NSF Research Experience for Undergraduates (REU); and sponsored research. Impact As metropolitan areas assume greater responsibility for driving social transformation and economic prosperity, the demand for publicly engaged and highly trained data scientists will continue to grow. For students eager to participate in this emerging field, the Data Science for Public Good program promises to deliver a unique set of resources and opportunities. World-Class Training: DSPG fellows participate in professional training and development workshops, poster presentations, and technical report and publication writing. Formal sessions on data science practices introduce students to scientific and statistical computing tools including R, Python, and GIS. Workshops also include accessing and using local, state and federal data resources such as Census products and open data portals. Professional Support: Each DSPG fellow is assigned a post-doctoral and senior researcher mentor. These mentors guide students’ research and facilitate discussions about future career opportunities, with a focus on public service. DSPG fellows also participate in strategic planning activities related to their projects including sponsor meetings, presentations, and other events. National Networks: Leveraging our laboratory’s location in the National Capital Region, excursions are organized to events at AAAS, national academies, congressional hearings, and a variety of Washington metropolitan science and technology activities. Fellows also attend the SDAL seminar series with national and international speakers from government, academia, and industry. To learn more about how our DSPG program is making a difference in the lives of students, read this profile on our inaugural cohort of summer research fellows. Program Sponsors: Who We Work With Every DSPG research project begins with a collaboration. While our sponsor organizations face unique challenges, they share a common commitment: leveraging data science to better serve the public good. Figure .: Nonexclusive list of sponsors Previous Projects Data Science for the Public Good Research Presentations Research from the Data Science for the Public Good (DSPG) program addresses fundamental problems faced by governmental agencies tasked with serving the public good, institutions ranging from the Department of Defense to emergency response services in our local county. The poster presentations below represent the range of research we’ve performed on behalf of each of our sponsors. You can find a list of previous presentations here: https://www.bi.vt.edu/sdal/careers/call-for-students/dspg-research-presentations "],
["lesson-materials.html", "Lesson Materials", " Lesson Materials The main source for the training materials come from Software-Carpentry (Wilson 2016), specifically the Bash, Git (Ahmadia et al. 2016), and SQL lessons. More references about Software-Carpentry and the challenges in scientific computing can be found here: (Wilson, n.d.) (Wilson 2009) (Hannay et al. 2009) (Wilson 2008) (Wilson 2006) (Wilson 2005) Software-Carpenty Data-Carpentry The Carpentries DataCamp R for Data Science By: Garrett Grolemund and Hadley Wickham http://r4ds.had.co.nz/ Figure .: R for Data Science Cover References "],
["setup.html", "Setup", " Setup In general, the only thing you really need to work on our servers is a browser. However, having a terminal application (e.g., Bash Shell) is really useful. The setup instructions are taken and adapted from the Software-Carpenty Workshop Template "],
["the-bash-shell.html", "The Bash Shell", " The Bash Shell Bash is a commonly-used shell that gives you the power to do simple tasks more quickly. Windows Video Tutorial Download the Git for Windows installer: https://git-for-windows.github.io/. Run the installer and follow the steps bellow: Click on “Next”. Click on “Next”. Keep “Use Git from the Windows Command Prompt” selected and click on “Next”. If you forgot to do this programs that you need for the workshop will not work properly. If this happens rerun the installer and select the appropriate option. Click on “Next”. Keep “Checkout Windows-style, commit Unix-style line endings” selected and click on “Next”. Keep “Use Windows’ default console window” selected and click on “Next”. Click on “Install”. Click on “Finish”. If your “HOME” environment variable is not set (or you don’t know what this is): Open command prompt (Open Start Menu then type cmd and press [Enter]) Type the following line into the command prompt window exactly as shown: setx HOME &quot;%USERPROFILE%&quot; Press [Enter], you should see SUCCESS: Specified value was saved. Quit command prompt by typing exit then pressing [Enter] This will provide you with both Git and Bash in the Git Bash program. macOS The default shell in all versions of macOS is Bash, so no need to install anything. You access Bash from the Terminal (found in /Applications/Utilities). See the Git installation video tutorial for an example on how to open the Terminal. You may want to keep Terminal in your dock for this workshop. Linux The default shell is usually Bash, but if your machine is set up differently you can run it by opening a terminal and typing bash. There is no need to install anything. "],
["git.html", "Git", " Git Git is a version control system that lets you track who made changes to what when and has options for easily updating a shared or public version of your code on github.com. You will need a supported web browser (current versions of Chrome, Firefox or Safari, or Internet Explorer version 9 or above). You will need an account at github.com for parts of the Git lesson. Basic GitHub accounts are free. We encourage you to create a GitHub account if you don’t have one already. Please consider what personal information you’d like to reveal. For example, you may want to review these instructions for keeping your email address private provided at GitHub. Windows Git should be installed on your computer as part of your Bash install (described above). macOS Video Tutorial For OS X 10.9 and higher, install Git for Mac by downloading and running the most recent “mavericks” installer from this list. After installing Git, there will not be anything in your /Applications folder, as Git is a command line program. For older versions of OS X (10.5-10.8) use the most recent available installer labelled “snow-leopard” available here. Linux If Git is not already available on your machine you can try to install it via your distro’s package manager. For Debian/Ubuntu run sudo apt-get install git and for Fedora run sudo dnf install git. "],
["text-editor.html", "Text Editor", " Text Editor When you’re writing code, it’s nice to have a text editor that is optimized for writing code, with features like automatic color-coding of key words. The default text editor on macOS and Linux is usually set to Vim, which is not famous for being intuitive. If you accidentally find yourself stuck in it, try typing the escape key, followed by :q! (colon, lower-case ‘q’, exclamation mark), then hitting Return to return to the shell. Windows Video Tutorial nano is a basic editor and the default that instructors use in the workshop. To install it, download the Windows installer and double click on the file to run it. This installer requires an active internet connection. Others editors that you can use are Notepad++ or Sublime Text. Be aware that you must add its installation directory to your system path. Please ask your instructor to help you do this. macOS nano is a basic editor and the default that instructors use in the workshop. See the Git installation video tutorial for an example on how to open nano. It should be pre-installed. Others editors that you can use are Text Wrangler or Sublime Text. Linux nano is a basic editor and the default that instructors use in the workshop. It should be pre-installed. Others editors that you can use are Gedit, Kate or Sublime Text. "],
["r.html", "R", " R R is a programming language that is especially powerful for data exploration, visualization, and statistical analysis. To interact with R, we use RStudio. Windows Video Tutorial Install R by downloading and running this .exe file from CRAN. Also, please install the RStudio IDE. Note that if you have separate user and admin accounts, you should run the installers as administrator (right-click on .exe file and select “Run as administrator” instead of double-clicking). Otherwise problems may occur later, for example when installing R packages. macOS Video Tutorial Install R by downloading and running this .pkg file from CRAN. Also, please install the RStudio IDE. Linux You can download the binary files for your distribution from CRAN. Or you can use your package manager (e.g. for Debian/Ubuntu run sudo apt-get install r-base and for Fedora run sudo dnf install R). Also, please install the RStudio IDE. "],
["sqlite.html", "SQLite", " SQLite SQL is a specialized programming language used with databases. We use a simple database manager called SQLite in our lessons. Windows The Windows Installer installs SQLite for Windows. If you used the installer to configure nano, you don’t need to run it again. macOS SQLite comes pre-installed on macOS. Linux SQLite comes pre-installed on Linux. "],
["ssh-keys.html", "SSH Keys", " SSH Keys It’ll be a good idea to first check and see if you already have any SSH keys: https://help.github.com/articles/checking-for-existing-ssh-keys/ GitHub has a set of instructions on how to create SSH keys: https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/#generating-a-new-ssh-key The below is an adapted form of the GiHub instructions Open Terminal. run ssh-keygen, this creates a new ssh key Generating public/private rsa key pair. When you’re prompted to “Enter a file in which to save the key,” press Enter. This accepts the default file location. Enter a file in which to save the key (/home/you/.ssh/id_rsa): [Press enter] At the prompt, when it asks for a password, just leave it blank and [Press enter]. Otherwise, for more information, see “Working with SSH key passphrases”. Enter passphrase (empty for no passphrase): [Type a passphrase] Enter same passphrase again: [Type passphrase again] If you look at the contents of ~/.ssh, you should see the id_rsa and id_rsa.pub files. $ ls ~/.ssh id_rsa id_rsa.pub Copy the public key (is_rsa.pub) $ cat ~/.ssh/id_rsa.pub This is the public key you will use to paste into the system that asks for an SSH key GitHub Once you have your ssh key copied, you can add the key to your GitHub account by following the GitHub instructions: https://help.github.com/articles/adding-a-new-ssh-key-to-your-github-account/ Lightfoot If you have an SSH key on your laptop, you can use it so you don’t have to type your password when logging into lightfoot run ssh-copy-id $ ssh-copy-id YOUR_PID@lightfoot.vbi.vt.edu It will prompt you for your password, and if it’s your first time connecting to the server also ask you to confirm the connection by typing yes Once you ran ssh-copy-id, you can connect directly to lightfoot $ ssh YOUR_PID@lightfoot.vbi.vt.edu GitLab (devlab) The process of adding a key to GitLab is similar to the GitHub instuctions: https://docs.gitlab.com/ee/gitlab-basics/create-your-ssh-keys.html "],
["syllabus.html", "Syllabus", " Syllabus "],
["lesson-breakdown.html", "Lesson Breakdown", " Lesson Breakdown Lesson 1 Goals: Get up and running in the lab Know what bits of infrastructure we have Introduction to R through visualization How to load your own data into R Getting setup Getting the shell SSH keys The SDAL Infrastructure RStudio The other tools availiable Shell (Bash) The “Tidyverse” ecosystem Scripts and running R code Exploring your data through visualizations (ggplot2) Loading and saving a datasets (readr, haven) Lesson 2 Goals: Introduction to Markdown (rmarkdown) and knitr Take “pretty” notes with “simple” notation Learn how to create basic reports with your code What the code projects look like Working with git locally Markdown Knitr The project template Workflow basics Projects Git (locally) Lesson 3 Goals: Git with an eye towards collaboration Working with remotes (GitHub and GitLab) Collaborating with branches Git (remotes) Git (branches) Git (collaboration) Lesson 4 Goals: Start the process of manipulating data Perform data subsetting and aggregations Explore data with basic statistics and visualizations Reshaping data and fixing common data problems through the tidying process Transform data with (dplyr) Pipes (%&gt;%) Exploratory Data Analysis (EDA) tibble, the tidyverse “dataframe” Tidying data (tidyr) Lesson 5 Goals: Understanding relational data Merging datasets together Work with relational data in a database Writing SQL code and how to run them from within R Relational Data in R (dplyr) Working with databases SQLite PostgreSQL SQL Working with SQL in your R code Lesson 6 Goals: Work with strings, factors, and date time values in R Strings Factors (forcats) Dates and Times Lesson 7 Programming “fundamentals” Functions Vectors Loops Functions Vectors Iteration purrr for loops Lesson 8 Goals: The dialects of R Review of tidyverse functions How tidyverse relates to base R The base R data.frame object apply family of functions How data.table playes a role in the R ecosystem base R data.table tidyverse Lesson 9 Working with geospatial data with sf Lesson 10 Goals: Web scraping API Scrape Lesson 11 Communication R Markdown Graphics R Markdown formats Shiny "],
["infrastructure.html", "Chapter 1 Infrastructure", " Chapter 1 Infrastructure We have 2 servers in the lab, lightfoot and snowmane (named after horses from Lord of the Rings). The main “production” server is lightfoot where all of our data and compute resources exist. "],
["components.html", "1.1 Components", " 1.1 Components We use Docker in our lab. This allows us to install various components without affecting the underlying server. A brief history of how we settled on docker and why is in this blog post on From VMs to LXC Containers to Docker Containers. Figure 1.1 depicts what conatiners we have on lightfoot. You can think of each ‘container’ as an ‘application’ just like one you are running on your laptop. But the behvaior of each ‘container’ is more like a separate server you connect to. Figure 1.1: The Docker infrastructure used in SDAL The primary conatiners you will be using are the RStudio, Adminer, and Django/Wagtail containers. They all exist on lightfoot and can all be reached in a browser with https://analytics.bi.vt.edu and an appropriate suffix (e.g., /rstudio, /db). "],
["accessing-servers.html", "1.2 Accessing Servers", " 1.2 Accessing Servers Your main point of contact will be using RStudio on lightfoot. There’s a few things that can be setup so you don’t have to type your password all the time. This involves creating “SSH keys”. Aside from creating keys, below is a set of links you’ll probably be using all the time: Rstudio Your own container: https://analytics.bi.vt.edu/YOUR_PID/rstudio It’s suggested you use the container assigned to yourself, since your work and crashed code is isolated from everyone else Generic RStudio container: https://analytics.bi.vt.edu/rstudio There is a generic container that can be used as well, it’s availiable to you, but if you have an individual conatiner, it’s better to use that one instead "],
["project-template.html", "1.3 Project Template", " 1.3 Project Template Why project templates: https://chendaniely.github.io/sdal/2017/05/30/project_templates/ Other resources: https://github.com/ropensci/rrrpkg https://github.com/benmarwick/rrtools A template for research projects structured as R packages: https://github.com/Pakillo/template project/ # the project/code repository | |- data/ # raw and primary data, are not changed once created | | | +- project_data/ # subfolder that links to an encrypted data storage container | | | | | |- original/ # raw data, will not be altered | | |- working/ # intermediate datasets from src code | | +- final/ # datasets used in analysis | | | +- more_data/ # some projects will need multiple links | |- src/ # any programmatic code | |- analysis1/ # user1 assigned to the project | +- analysis2/ # user2 assigned to the project | |- R/ # functions | |- tests/ # unit tests | |- output # all output and results from workflows and analyses | |- figures/ # graphs, likely designated for manuscript figures | |- pictures/ # diagrams, images, and other non-graph graphics | +- analysis/ # generated reports for (e.g. rmarkdown output) | |- README.md # the top level description of content | |- .gitignore # git ignore file |- project.Rproj # RStudio project | |- DESCRIPTION # Description file to repo into R package, if applicable +- Makefile # Makefile, if applicable Notes: you can find our project template .gitignore file here: https://github.com/bi-sdal/project_template/blob/master/gitignore "],
["git-1.html", "Chapter 2 Git", " Chapter 2 Git Software-Carpentry Git Lesson DataCamp Courses: Introduction to Git for Data Science Working with the RStudio IDE (Part 2) – Chapter 2: Version Control Quick References: Software-Carpentry Reference Git Cheat Sheet (by Github) Jenny Bryan’s “Happy Git and GitHub for the useR” Git interaction from NDP Software Learn Git Branching "],
["git-on-your-own.html", "2.1 Git on your own", " 2.1 Git on your own Figure 2.1: Diagram of Git commands and how they relate to one another. How not to write commit messages: how #not to write #git #commit messages -.-'' pic.twitter.com/5TdiZ1yi5S — Dⓐniel Chen ((???)) April 16, 2015 "],
["git-with-branches.html", "2.2 Git with branches", " 2.2 Git with branches Figure 2.2: Review of Git Figure 2.3: What branching looks like in the Git world "],
["collaborating-with-git.html", "2.3 Collaborating with Git", " 2.3 Collaborating with Git Figure .: The ‘forking’ model of Git workflows Figure .: Git with branches "],
["protecting-branches.html", "2.4 Protecting branches", " 2.4 Protecting branches https://docs.gitlab.com/ee/user/project/protected_branches.html In a repository go to settings &gt; repository &gt; protected branches set “allowed to merge”: masters “allowed to push”: no one If you accidently did work on master: create a branch where you are now: git branch BRANCH_NAME reset master to where you were: git reset --hard COMMIT_HASH_FOR_MASTER make sure you do this on the master branch go to your branch: git checkout BRANCH_NAME push your branch: git push origin BRANCH_NAME create and merge the pull/merge request "],
["build-details.html", "Build Details", " Build Details Sys.time() ## [1] &quot;2018-05-25 15:06:21 EDT&quot; sessionInfo() ## R version 3.5.0 (2018-04-23) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Arch Linux ## ## Matrix products: default ## BLAS: /usr/lib/libblasp-r0.2.20.so ## LAPACK: /usr/lib/liblapack.so.3.8.0 ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_3.5.0 backports_1.1.2 bookdown_0.7 magrittr_1.5 ## [5] rprojroot_1.3-2 tools_3.5.0 htmltools_0.3.6 yaml_2.1.19 ## [9] Rcpp_0.12.17 stringi_1.2.2 rmarkdown_1.9 knitr_1.20 ## [13] xfun_0.1 stringr_1.3.1 digest_0.6.15 evaluate_0.10.1 devtools::session_info() ## Session info ------------------------------------------------------------- ## setting value ## version R version 3.5.0 (2018-04-23) ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## tz America/New_York ## date 2018-05-25 ## Packages ----------------------------------------------------------------- ## package * version date source ## backports 1.1.2 2017-12-13 CRAN (R 3.5.0) ## base * 3.5.0 2018-04-23 local ## bookdown 0.7 2018-02-18 CRAN (R 3.5.0) ## compiler 3.5.0 2018-04-23 local ## datasets * 3.5.0 2018-04-23 local ## devtools 1.13.5 2018-02-18 CRAN (R 3.5.0) ## digest 0.6.15 2018-01-28 CRAN (R 3.5.0) ## evaluate 0.10.1 2017-06-24 CRAN (R 3.5.0) ## graphics * 3.5.0 2018-04-23 local ## grDevices * 3.5.0 2018-04-23 local ## htmltools 0.3.6 2017-04-28 CRAN (R 3.5.0) ## knitr 1.20 2018-02-20 CRAN (R 3.5.0) ## magrittr 1.5 2014-11-22 CRAN (R 3.5.0) ## memoise 1.1.0 2017-04-21 CRAN (R 3.5.0) ## methods * 3.5.0 2018-04-23 local ## Rcpp 0.12.17 2018-05-18 CRAN (R 3.5.0) ## rmarkdown 1.9 2018-03-01 CRAN (R 3.5.0) ## rprojroot 1.3-2 2018-01-03 CRAN (R 3.5.0) ## stats * 3.5.0 2018-04-23 local ## stringi 1.2.2 2018-05-02 CRAN (R 3.5.0) ## stringr 1.3.1 2018-05-10 CRAN (R 3.5.0) ## tools 3.5.0 2018-04-23 local ## utils * 3.5.0 2018-04-23 local ## withr 2.1.2 2018-03-15 CRAN (R 3.5.0) ## xfun 0.1 2018-01-22 CRAN (R 3.5.0) ## yaml 2.1.19 2018-05-01 CRAN (R 3.5.0) "],
["references.html", "References", " References "]
]
