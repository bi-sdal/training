[
["index.html", "Training Manual for SDAL Welcome", " Training Manual for SDAL Welcome The purpose of this website is to consolodate all of the ‘basic’ trainings and knowledge in the lab to be used as a reference and trainining handbook used in our summer Data Science for the Public Good Program. "],
["the-social-and-decision-analytics-laboratory.html", "The Social and Decision Analytics Laboratory", " The Social and Decision Analytics Laboratory The Social and Decision Analytics Laboratory brings together statisticians and social and behavioral scientists to embrace today’s data revolution, developing evidence-based research and quantitative methods to inform policy decision-making. https://www.bi.vt.edu/sdal/about Overview: Our Mission &amp; Methods As a leading laboratory in the Biocomplexity Institute of Virginia Tech, the Social and Decision Analytics Laboratory is modeling the social condition of metropolitan areas, integrating novel sources of data to examine health and wellness, and uncovering key factors that drive industrial innovation. Social and decision analytics have the power to fundamentally change our understanding of the world around us. Our research informs effective policy-making under uncertainty, combining expertise in statistics and the social sciences to transform all data into actionable knowledge. Leveraging analytics to solve real-world problems requires a broad range of expertise. The SDAL team includes thought leaders in a wide variety of fields: statistics, social, behavioral and economic sciences, political science, psychology, simulation, computer science, data analytics, information technology policy, and infrastructural resilience. SDAL is driven to make data analytics an accessible and effective part of the policy-making process. SDAL’s newest partnerships and recent discoveries are regularly profiled on our news page, while public lectures are available in our presentations archive. Our location at the Virginia Tech Research Center in Arlington, Virginia allows us to partner with local, state, and federal governments, non-profit organizations, and industry throughout the D.C. metro area and beyond. Innovation: Our Research Focus Areas Every global challenge we face today is rooted in information. Advances in big data analysis have the potential to revolutionize the ways we address long-standing social problems such as unequal access to services and affordable housing. To address these and other pressing issues, the Social and Decision Analytics Laboratory is pioneering three major domains of research. The Science of “All” Data aims to integrate disparate sources of data—surveys and experiments; local, state, and federal administrative government records; social media; mobile technology, and geographic information—to deliver a comprehensive understanding of social problems. We are experts in drawing actionable information out of messy data sets of all sizes. Information Diffusion Analytics charts how knowledge and social attitudes spread through information networks using a combination of network science, cognitive modeling, crowd-sourcing, and machine learning. Our research predicts how information reaches people and affects their outlook. Community Learning Data-Driven Discovery (CLD3) allows organizations to identify individually maintained data resources that serve their mutual needs. Information that would have been siloed within a single department is “liberated,” freed for use throughout the community. Our research focuses on how to discovery, liberate, and repurpose these data flows to achieve data-informed policy development. Our unique CLD3 approach is supported by expertise in five sub-domains of analytics research: Metropolitan Analytics applies data on infrastructure, environment, and people to understand how urban areas can support the well-being of their expanding populations. Our research guides the growth of resilient cities. Education and Labor Force Analytics seeks to provide a clearer picture of the factors that help students and employees thrive by understanding learning processes, social interactions, and learning environments. Our work examines the impact of education on labor force outcomes. Health and Well Being Analytics leverages public and private sector medical data to help communities deliver healthcare services where they are most needed. SDAL studies help healthcare resources do more good. Emergency Management Analytics creates deep characterizations of situational awareness at all levels through accessing and integrating comprehensive information of conditions that increase risks and stress to the emergency responders and the citizens they serve. Our research guides the development of equitable policies and practices to improve the safety and security of our communities. Industrial Innovation Analytics utilizes data from partners in the private sector to identify factors that reduce costs and improve efficiency. We enable the private sector to move beyond business analytics to develop information integration technologies. Collaboration: Our Research Partnerships Since its founding in 2013, SDAL has developed its world-class statistical and data science capabilities in support of the Biocomplexity Institute’s overarching mission to predict, explain, and visualize the behavior of massively interacting systems. Backed by the Biocomplexity Institute’s diverse research programs and unique, high-performance computing infrastructure, SDAL has established long-term partnerships with a variety of organizations including Arlington County, Procter &amp; Gamble, the Robert Wood Johnson Foundation, the United States Census Bureau, and the Department of Housing and Urban Development. The Social and Decision Analytics Laboratory welcomes new collaboration opportunities. To inquire, please contact our director, Sallie Keller. "],
["data-science-for-the-public-good-program.html", "Data Science for the Public Good Program", " Data Science for the Public Good Program The Data Science for the Public Good program (DSPG) engages young scholars in finding solutions to some of the most pressing social issues of our time. DSPG fellows conduct research at the intersection of statistics, computation, and the social sciences to determine how information generated within every community can be leveraged to improve quality of life. Problem Our communities are engaged in a tremendous struggle to manage conflicting forces that threaten their ability to evolve and thrive. Government officials and policymakers must continuously develop their capacity to provide health, safety, security, employment, and leisure to their constituents. Sustaining this level of growth is becoming increasingly difficult in a climate of diminishing resources, mounting inequality, rapid technological change, and expanding global networks. Data-driven research provides a rich, mutually rewarding opportunity to leverage community knowledge and public information resources to affect positive social change. The Data Science for the Public Good program was established to connect aspiring data science scholars to communities which can benefit from their expertise. Students selected through a competitive application process are engaged in a series of hands-on learning experiences while policymakers and government leaders receive data analysis support to inform difficult decisions related to healthcare, education, and social justice. The story of each community—its problems, needs, and aspirations—is contained within its data. The DSPG program’s overarching objective is to equip new generations of scientists with the skills they need to bring this story to light for leaders in local government. Methods The Data Science for the Public Good program teaches student fellows how to sift through vast amounts of information related to public safety, employment, and the provision of services to discover how communities can become more efficient and sustainable. Through the lenses of statistics, social science, and data science research, DSPG students will learn to integrate all available data resources in order to: Identify pressing issues through direct engagement with government and community leaders. Develop mechanisms to assist decision-makers in framing their large-scale policy questions and identifying data sources which can be leveraged to address these issues at the local, state, and federal levels. Create a two-way data pipeline to give local leaders a direct link to cutting-edge scientific analyses and researchers easier access to local, state, and federal data flows. DSPG fellows are also given opportunities to diversify their expertise and form lasting professional connections by taking part in the Social and Decision Analytics Laboratory’s data-driven research projects. Our research teams are: Horizontally integrated, combining disciplines including statistics, data science, and the social and behavioral sciences to address complex issues. Vertically integrated, allowing students to collaborate with project stakeholders at all levels including undergraduates, graduates, postdoctoral associates, research faculty, and local, state, and federal agency leadership. These unique fellowship experiences are made possible through the support of several research organizations dedicated to serving the public good: Virginia Tech’s Global Forum for Urban and Regional Resilience (GFURR); American Statistical Association’s NSF Research Experience for Undergraduates (REU); and sponsored research. Impact As metropolitan areas assume greater responsibility for driving social transformation and economic prosperity, the demand for publicly engaged and highly trained data scientists will continue to grow. For students eager to participate in this emerging field, the Data Science for Public Good program promises to deliver a unique set of resources and opportunities. World-Class Training: DSPG fellows participate in professional training and development workshops, poster presentations, and technical report and publication writing. Formal sessions on data science practices introduce students to scientific and statistical computing tools including R, Python, and GIS. Workshops also include accessing and using local, state and federal data resources such as Census products and open data portals. Professional Support: Each DSPG fellow is assigned a post-doctoral and senior researcher mentor. These mentors guide students’ research and facilitate discussions about future career opportunities, with a focus on public service. DSPG fellows also participate in strategic planning activities related to their projects including sponsor meetings, presentations, and other events. National Networks: Leveraging our laboratory’s location in the National Capital Region, excursions are organized to events at AAAS, national academies, congressional hearings, and a variety of Washington metropolitan science and technology activities. Fellows also attend the SDAL seminar series with national and international speakers from government, academia, and industry. To learn more about how our DSPG program is making a difference in the lives of students, read this profile on our inaugural cohort of summer research fellows. Program Sponsors: Who We Work With Every DSPG research project begins with a collaboration. While our sponsor organizations face unique challenges, they share a common commitment: leveraging data science to better serve the public good. Figure .: Nonexclusive list of sponsors Previous Projects Data Science for the Public Good Research Presentations Research from the Data Science for the Public Good (DSPG) program addresses fundamental problems faced by governmental agencies tasked with serving the public good, institutions ranging from the Department of Defense to emergency response services in our local county. The poster presentations below represent the range of research we’ve performed on behalf of each of our sponsors. You can find a list of previous presentations here: https://www.bi.vt.edu/sdal/careers/call-for-students/dspg-research-presentations "],
["lesson-materials.html", "Lesson Materials", " Lesson Materials The main source for the training materials come from Software-Carpentry (Wilson 2016), specifically the Bash, Git (Ahmadia et al. 2016), and SQL lessons. More references about Software-Carpentry and the challenges in scientific computing can be found here: (Wilson, n.d.) (Wilson 2009) (Hannay et al. 2009) (Wilson 2008) (Wilson 2006) (Wilson 2005) Software-Carpenty Data-Carpentry The Carpentries DataCamp R for Data Science By: Garrett Grolemund and Hadley Wickham http://r4ds.had.co.nz/ Figure .: R for Data Science Cover References "],
["setup.html", "Setup", " Setup In general, the only thing you really need to work on our servers is a browser. However, having a terminal application (e.g., Bash Shell) is really useful. The setup instructions are taken and adapted from the Software-Carpenty Workshop Template "],
["the-bash-shell.html", "The Bash Shell", " The Bash Shell Bash is a commonly-used shell that gives you the power to do simple tasks more quickly. Windows Video Tutorial Download the Git for Windows installer: https://git-for-windows.github.io/. Run the installer and follow the steps bellow: Click on “Next”. Click on “Next”. Keep “Use Git from the Windows Command Prompt” selected and click on “Next”. If you forgot to do this programs that you need for the workshop will not work properly. If this happens rerun the installer and select the appropriate option. Click on “Next”. Keep “Checkout Windows-style, commit Unix-style line endings” selected and click on “Next”. Keep “Use Windows’ default console window” selected and click on “Next”. Click on “Install”. Click on “Finish”. If your “HOME” environment variable is not set (or you don’t know what this is): Open command prompt (Open Start Menu then type cmd and press [Enter]) Type the following line into the command prompt window exactly as shown: setx HOME &quot;%USERPROFILE%&quot; Press [Enter], you should see SUCCESS: Specified value was saved. Quit command prompt by typing exit then pressing [Enter] This will provide you with both Git and Bash in the Git Bash program. macOS The default shell in all versions of macOS is Bash, so no need to install anything. You access Bash from the Terminal (found in /Applications/Utilities). See the Git installation video tutorial for an example on how to open the Terminal. You may want to keep Terminal in your dock for this workshop. Linux The default shell is usually Bash, but if your machine is set up differently you can run it by opening a terminal and typing bash. There is no need to install anything. "],
["git.html", "Git", " Git Git is a version control system that lets you track who made changes to what when and has options for easily updating a shared or public version of your code on github.com. You will need a supported web browser (current versions of Chrome, Firefox or Safari, or Internet Explorer version 9 or above). You will need an account at github.com for parts of the Git lesson. Basic GitHub accounts are free. We encourage you to create a GitHub account if you don’t have one already. Please consider what personal information you’d like to reveal. For example, you may want to review these instructions for keeping your email address private provided at GitHub. Windows Git should be installed on your computer as part of your Bash install (described above). macOS Video Tutorial For OS X 10.9 and higher, install Git for Mac by downloading and running the most recent “mavericks” installer from this list. After installing Git, there will not be anything in your /Applications folder, as Git is a command line program. For older versions of OS X (10.5-10.8) use the most recent available installer labelled “snow-leopard” available here. Linux If Git is not already available on your machine you can try to install it via your distro’s package manager. For Debian/Ubuntu run sudo apt-get install git and for Fedora run sudo dnf install git. "],
["text-editor.html", "Text Editor", " Text Editor When you’re writing code, it’s nice to have a text editor that is optimized for writing code, with features like automatic color-coding of key words. The default text editor on macOS and Linux is usually set to Vim, which is not famous for being intuitive. If you accidentally find yourself stuck in it, try typing the escape key, followed by :q! (colon, lower-case ‘q’, exclamation mark), then hitting Return to return to the shell. Atom: https://flight-manual.atom.io/getting-started/sections/installing-atom/ Sublime Text: https://www.sublimetext.com/3 Emacs VIM Windows Video Tutorial nano is a basic editor and the default that instructors use in the workshop. To install it, download the Windows installer and double click on the file to run it. This installer requires an active internet connection. Others editors that you can use are Notepad++ or Sublime Text. Be aware that you must add its installation directory to your system path. Please ask your instructor to help you do this. macOS nano is a basic editor and the default that instructors use in the workshop. See the Git installation video tutorial for an example on how to open nano. It should be pre-installed. Others editors that you can use are Text Wrangler or Sublime Text. Linux nano is a basic editor and the default that instructors use in the workshop. It should be pre-installed. Others editors that you can use are Gedit, Kate or Sublime Text. "],
["r.html", "R", " R R is a programming language that is especially powerful for data exploration, visualization, and statistical analysis. To interact with R, we use RStudio. RStudio IDE: https://www.rstudio.com/products/rstudio/download/preview/ Windows Video Tutorial Install R by downloading and running this .exe file from CRAN. Also, please install the RStudio IDE. Note that if you have separate user and admin accounts, you should run the installers as administrator (right-click on .exe file and select “Run as administrator” instead of double-clicking). Otherwise problems may occur later, for example when installing R packages. macOS Video Tutorial Install R by downloading and running this .pkg file from CRAN. Also, please install the RStudio IDE. Linux You can download the binary files for your distribution from CRAN. Or you can use your package manager (e.g. for Debian/Ubuntu run sudo apt-get install r-base and for Fedora run sudo dnf install R). Also, please install the RStudio IDE. "],
["sqlite.html", "SQLite", " SQLite SQL is a specialized programming language used with databases. We use a simple database manager called SQLite in our lessons. Windows The Windows Installer installs SQLite for Windows. If you used the installer to configure nano, you don’t need to run it again. macOS SQLite comes pre-installed on macOS. Linux SQLite comes pre-installed on Linux. "],
["ssh-keys.html", "SSH Keys", " SSH Keys Typically we’ll be going over this step together as a class. As long as you have Bash installed you’ll be okay. It’ll be a good idea to first check and see if you already have any SSH keys: https://help.github.com/articles/checking-for-existing-ssh-keys/ GitHub has a set of instructions on how to create SSH keys: https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/#generating-a-new-ssh-key The below is an adapted form of the GiHub instructions Open Terminal. run ssh-keygen, this creates a new ssh key Generating public/private rsa key pair. When you’re prompted to “Enter a file in which to save the key,” press Enter. This accepts the default file location. Enter a file in which to save the key (/home/you/.ssh/id_rsa): [Press enter] At the prompt, when it asks for a password, just leave it blank and [Press enter]. Otherwise, for more information, see “Working with SSH key passphrases”. Enter passphrase (empty for no passphrase): [Type a passphrase] Enter same passphrase again: [Type passphrase again] If you look at the contents of ~/.ssh, you should see the id_rsa and id_rsa.pub files. $ ls ~/.ssh id_rsa id_rsa.pub Copy the public key (is_rsa.pub) $ cat ~/.ssh/id_rsa.pub This is the public key you will use to paste into the system that asks for an SSH key GitHub Once you have your ssh key copied, you can add the key to your GitHub account by following the GitHub instructions: https://help.github.com/articles/adding-a-new-ssh-key-to-your-github-account/ Lightfoot If you have an SSH key on your laptop, you can use it so you don’t have to type your password when logging into lightfoot run ssh-copy-id $ ssh-copy-id YOUR_PID@lightfoot.vbi.vt.edu It will prompt you for your password, and if it’s your first time connecting to the server also ask you to confirm the connection by typing yes Once you ran ssh-copy-id, you can connect directly to lightfoot $ ssh YOUR_PID@lightfoot.vbi.vt.edu GitLab (devlab) The process of adding a key to GitLab is similar to the GitHub instuctions: https://docs.gitlab.com/ee/gitlab-basics/create-your-ssh-keys.html "],
["latex.html", "LaTeX", " LaTeX Please go to https://www.latex-project.org/get/ to find the LaTeX distribution for your operating system. If you can, you should try to install the “full” version of TeX, but it is a very large install. The upside of installing everything is usually you’ll never have a problem in the future with loading TeX or LaTeX packages. There’s also a very good LaTeX IDE called TeXstudio: https://www.texstudio.org/. It’s a fork of Texmaker (http://www.xm1math.net/texmaker/), but I find that TeXstudio has better autocompletes. You can try both of them, they’re pretty much the same (in both function and visuals) "],
["syllabus.html", "Syllabus", " Syllabus "],
["lesson-breakdown.html", "Lesson Breakdown", " Lesson Breakdown Lesson 1 Goals: Get up and running in the lab Know what bits of infrastructure we have Introduction to R through visualization How to load your own data into R Getting setup Getting the shell SSH keys The SDAL Infrastructure RStudio The other tools availiable Shell (Bash) The “Tidyverse” ecosystem Scripts and running R code Exploring your data through visualizations (ggplot2) Loading and saving a datasets (readr, haven) Lesson 2 Goals: Introduction to Markdown (rmarkdown) and knitr Take “pretty” notes with “simple” notation Learn how to create basic reports with your code What the code projects look like Working with git locally Markdown Knitr The project template Workflow basics Projects Git (locally) Lesson 3 Goals: Git with an eye towards collaboration Working with remotes (GitHub and GitLab) Collaborating with branches Git (remotes) Git (branches) Git (collaboration) Lesson 4 Goals: Start the process of manipulating data Perform data subsetting and aggregations Explore data with basic statistics and visualizations Reshaping data and fixing common data problems through the tidying process Transform data with (dplyr) Pipes (%&gt;%) Exploratory Data Analysis (EDA) tibble, the tidyverse “dataframe” Tidying data (tidyr) Lesson 5 Goals: Understanding relational data Merging datasets together Work with relational data in a database Writing SQL code and how to run them from within R Relational Data in R (dplyr) Working with databases SQLite PostgreSQL SQL Working with SQL in your R code Lesson 6 Goals: Work with strings, factors, and date time values in R Strings Factors (forcats) Dates and Times Lesson 7 Programming “fundamentals” Functions Vectors Loops Functions Vectors Iteration purrr for loops Lesson 8 Goals: The dialects of R Review of tidyverse functions How tidyverse relates to base R The base R data.frame object apply family of functions How data.table playes a role in the R ecosystem base R data.table tidyverse Lesson 9 Working with geospatial data with sf Lesson 10 Goals: Web scraping API Scrape Lesson 11 Communication R Markdown Graphics R Markdown formats Shiny "],
["infrastructure.html", "Chapter 1 Infrastructure", " Chapter 1 Infrastructure We have 2 servers in the lab, lightfoot and snowmane (named after horses from Lord of the Rings). The main “production” server is lightfoot where all of our data and compute resources exist. "],
["components.html", "1.1 Components", " 1.1 Components We use Docker in our lab. This allows us to install various components without affecting the underlying server. A brief history of how we settled on docker and why is in this blog post on From VMs to LXC Containers to Docker Containers. Figure 1.1 depicts what conatiners we have on lightfoot. You can think of each ‘container’ as an ‘application’ just like one you are running on your laptop. But the behvaior of each ‘container’ is more like a separate server you connect to. Figure 1.1: The Docker infrastructure used in SDAL. The containers on the top row are the parts of the system lab members will be connecting to and working on. The primary conatiners you will be using are the RStudio, Adminer, and Django/Wagtail containers. They all exist on lightfoot and can all be reached in a browser with https://analytics.bi.vt.edu and an appropriate suffix (e.g., /rstudio, /db). "],
["accessing-servers.html", "1.2 Accessing Servers", " 1.2 Accessing Servers Your main point of contact will be using RStudio on lightfoot. There’s a few things that can be setup so you don’t have to type your password all the time. This involves creating “SSH keys”. Aside from creating keys, below is a set of links you’ll probably be using all the time: Rstudio Your own container: https://analytics.bi.vt.edu/YOUR_PID/rstudio It’s suggested you use the container assigned to yourself, since your work and crashed code is isolated from everyone else Generic RStudio container: https://analytics.bi.vt.edu/rstudio There is a generic container that can be used as well, it’s availiable to you, but if you have an individual conatiner, it’s better to use that one instead "],
["project-template.html", "1.3 Project Template", " 1.3 Project Template Why project templates: https://chendaniely.github.io/sdal/2017/05/30/project_templates/ Other resources: https://github.com/ropensci/rrrpkg https://github.com/benmarwick/rrtools A template for research projects structured as R packages: https://github.com/Pakillo/template project/ # the project/code repository | |- data/ # raw and primary data, are not changed once created | | | +- project_data/ # subfolder that links to an encrypted data storage container | | | | | |- original/ # raw data, will not be altered | | |- working/ # intermediate datasets from src code | | +- final/ # datasets used in analysis | | | +- more_data/ # some projects will need multiple links | |- src/ # any programmatic code | |- analysis1/ # user1 assigned to the project | +- analysis2/ # user2 assigned to the project | |- R/ # functions | |- tests/ # unit tests | |- output # all output and results from workflows and analyses | |- figures/ # graphs, likely designated for manuscript figures | |- pictures/ # diagrams, images, and other non-graph graphics | +- analysis/ # generated reports for (e.g. rmarkdown output) | |- README.md # the top level description of content | |- .gitignore # git ignore file |- project.Rproj # RStudio project | |- DESCRIPTION # Description file to repo into R package, if applicable +- Makefile # Makefile, if applicable Notes: you can find our project template .gitignore file here: https://github.com/bi-sdal/project_template/blob/master/gitignore "],
["data-visualization.html", "Chapter 2 Data Visualization", " Chapter 2 Data Visualization Data visualization Chapter in r4ds http://r4ds.had.co.nz/data-visualisation.html Datacamp Courses: The ggplot2 stack https://www.datacamp.com/courses/data-visualization-with-ggplot2-1 https://www.datacamp.com/courses/data-visualization-with-ggplot2-2 https://www.datacamp.com/courses/data-visualization-with-ggplot2-part-3 "],
["loading-ggplot2.html", "2.1 Loading ggplot2", " 2.1 Loading ggplot2 library(ggplot2) # mpg dataset from the ggplot2 library mpg ## # A tibble: 234 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto(l… f 18 29 p ## 2 audi a4 1.8 1999 4 manual… f 21 29 p ## 3 audi a4 2 2008 4 manual… f 20 31 p ## 4 audi a4 2 2008 4 auto(a… f 21 30 p ## 5 audi a4 2.8 1999 6 auto(l… f 16 26 p ## 6 audi a4 2.8 1999 6 manual… f 18 26 p ## 7 audi a4 3.1 2008 6 auto(a… f 18 27 p ## 8 audi a4 quat… 1.8 1999 4 manual… 4 18 26 p ## 9 audi a4 quat… 1.8 1999 4 auto(l… 4 16 25 p ## 10 audi a4 quat… 2 2008 4 manual… 4 20 28 p ## # ... with 224 more rows, and 1 more variable: class &lt;chr&gt; "],
["creating-a-ggplot.html", "2.2 Creating a ggplot", " 2.2 Creating a ggplot ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) "],
["aesthetic-mapings.html", "2.3 Aesthetic Mapings", " 2.3 Aesthetic Mapings # using color ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = class)) # using size ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, size = class)) ## Warning: Using size for a discrete variable is not advised. # using alpha ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, alpha = class)) # using shape ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, shape = class)) ## Warning: The shape palette can deal with a maximum of 6 discrete values ## because more than 6 becomes difficult to discriminate; you have 7. ## Consider specifying shapes manually if you must have them. ## Warning: Removed 62 rows containing missing values (geom_point). # manual set property # note color is not in the aes ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy), color = &quot;blue&quot;) # if you put a variable outside you will get an error ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy), color = class) "],
["facets.html", "2.4 Facets", " 2.4 Facets ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) + facet_wrap(~ class, nrow = 2) ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) + facet_grid(drv ~ cyl) "],
["geometic-objects.html", "2.5 Geometic Objects", " 2.5 Geometic Objects ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy)) ## `geom_smooth()` using method = &#39;loess&#39; ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy, linetime = drv)) ## Warning: Ignoring unknown aesthetics: linetime ## `geom_smooth()` using method = &#39;loess&#39; # base plot before groupings ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy)) ## `geom_smooth()` using method = &#39;loess&#39; # base plot before groupings ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy)) ## `geom_smooth()` using method = &#39;loess&#39; # separate smoothing line by group ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy, group = drv)) ## `geom_smooth()` using method = &#39;loess&#39; # different color foe each group ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy, color = drv), show.legend = FALSE) ## `geom_smooth()` using method = &#39;loess&#39; Adding multiple geoms in the same plot ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) + geom_smooth(mapping = aes(x = displ, y = hwy)) ## `geom_smooth()` using method = &#39;loess&#39; The layering system will carry over values from the previous layer. the ggplot layer will specify the global values ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; Mappings in a a geom function, will overwrite the global settings (i.e., they are local settings) ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point(mapping = aes(color = class)) + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point(mapping = aes(color = class)) + geom_smooth( data = dplyr::filter(mpg, class == &#39;subcompact&#39;), se = FALSE ) ## `geom_smooth()` using method = &#39;loess&#39; "],
["statistical-transformations.html", "2.6 Statistical Transformations", " 2.6 Statistical Transformations dim(diamonds) ## [1] 53940 10 head(diamonds) ## # A tibble: 6 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut)) Use a stat to calculate a new value. diamonds data gets “transformed” into a frequency table that get’s plotted by the bar plot. Look at the geom_bar documentation, you will see the stat will be count (i.e., stat_count()). ggplot(data = diamonds) + stat_count(mapping = aes(x = cut)) You can set stat to ‘identity’ if you have already calculated a frequency table pre_counted &lt;- tibble::as.tibble(table(diamonds$cut)) ggplot(data = pre_counted) + geom_bar( mapping = aes(x = Var1, y = n), stat = &#39;identity&#39; ) # overwrite default stat # proportion instead of count ggplot(data = diamonds) + geom_bar( mapping = aes(x = cut, y = ..prop.., group = 1) ) grouping: http://ggplot2.tidyverse.org/reference/aes_group_order.html By default, the group is set to the interaction of all discrete variables in the plot. This often partitions the data correctly, but when it does not, or when no discrete variable is used in the plot, you will need to explicitly define the grouping structure, by mapping group to a variable that has a different value for each group. "],
["position-adjustments.html", "2.7 Position Adjustments", " 2.7 Position Adjustments # using color ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, color = cut)) # using fill ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = cut)) # another example of fill # fill a different variable than x # This creates a stacked bar chart ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = clarity)) position: identity ggplot(data = diamonds, mapping = aes(x = cut, fill = clarity)) + geom_bar(alpha = 1/5, position = &#39;identity&#39;) ggplot(data = diamonds, mapping = aes(x = cut, color = clarity)) + geom_bar(fill = NA, position = &#39;identity&#39;) potition: fill ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = clarity), position = &#39;fill&#39;) position: dodge ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = clarity), position = &#39;dodge&#39;) Jitter scatter plot ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy), position = &#39;jitter&#39;) "],
["coordinate-systems.html", "2.8 Coordinate Systems", " 2.8 Coordinate Systems coord_flipswaps the x and y axis, useful when you have long labels ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + geom_boxplot() ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + geom_boxplot() + coord_flip() coord_quickmap, sets aspect ratio for maps usa &lt;- ggplot2::map_data(&#39;usa&#39;) ggplot(usa, aes(long, lat, group = group)) + geom_polygon(fill = &#39;white&#39;, color = &#39;black&#39;) ggplot(usa, aes(long, lat, group = group)) + geom_polygon(fill = &#39;white&#39;, color = &#39;black&#39;) + coord_quickmap() corrd_polar, uses polar coordinates bar &lt;- ggplot(data = diamonds) + geom_bar(mapping = aes(x = cut, fill = cut), show.legend = FALSE, width = 1) + theme(aspect.ratio = 1) + labs(x = NULL, y = NULL) bar + coord_flip() bar + coord_polar() "],
["importing-data.html", "Chapter 3 Importing Data", " Chapter 3 Importing Data Data import chapter in r4ds http://r4ds.had.co.nz/data-import.html DataCamp Courses: https://www.datacamp.com/courses/importing-data-in-r-part-1 https://www.datacamp.com/courses/importing-data-in-r-part-2 "],
["loading-datasets.html", "3.1 Loading datasets", " 3.1 Loading datasets library(readr) look at read_ set of functions heights = read_csv(&#39;data/heights.csv&#39;) ## Parsed with column specification: ## cols( ## earn = col_double(), ## height = col_double(), ## sex = col_character(), ## ed = col_integer(), ## age = col_integer(), ## race = col_character() ## ) skip number of rows comments can be skipped col_names for no column names manually pass in column names na specifies what is “missing” in base R, read_csv, data.table::fread() string as factors! useing readr compared to base R "],
["writing-to-a-files.html", "3.2 Writing to a files", " 3.2 Writing to a files readr has a write_csv and write_tsv function Use write_rds and read_rds (similar to readRDS and saveRDS) feather can be used to share data between languages (e.g., Python) haven: SPSS, Stata, and SAS files readxl: excel files (xls, xlsx) DBI + backend (RMySQL, RSQLite, RPostgreSQL, etc) to connect to databases jsonlite xml2 rio "],
["data-transformations.html", "Chapter 4 Data Transformations", " Chapter 4 Data Transformations Data Transformation chapter in r4ds http://r4ds.had.co.nz/transform.html DataCamp Courses: https://www.datacamp.com/courses/dplyr-data-manipulation-r-tutorial https://www.datacamp.com/courses/introduction-to-the-tidyverse https://www.datacamp.com/courses/cleaning-data-in-r References: dplyr vignette: https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html https://dplyr.tidyverse.org/ Rstudio Data Transformation Cheat Sheet Tidyverse for beginners DataCamp Cheatsheet "],
["dplyr-library.html", "4.1 dplyr library", " 4.1 dplyr library library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(nycflights13) flights ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # ... with 336,766 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; The basic verbs in dplyr filter(): selects rows arrange(): reorders rows select(): selects (and re-order) columns mutate(): create new variables (columns) based on existing variables (columns) summarise(): collapses multiple values into a single value (e.g., mean, standard deviation, etc) "],
["filter.html", "4.2 Filter", " 4.2 Filter filter(flights, month == 1) ## # A tibble: 27,004 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # ... with 26,994 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; filter(flights, month == 1, day == 1) ## # A tibble: 842 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # ... with 832 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; jan1 &lt;- filter(flights, month == 1, day == 1) # jan1 = filter(flights, month == 1, day == 1) (jan1 &lt;- filter(flights, month == 1, day == 1)) ## # A tibble: 842 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # ... with 832 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; "],
["comparison-operators.html", "4.3 Comparison (operators)", " 4.3 Comparison (operators) Comparison operators are used to compare 2 values to one another &gt;: greater than &gt;=: greater than or equal to &lt;: less than &lt;=: less than or equal to !=: not equal ==: used to compare if two things are equal Be careful when trying to compare things that are calculations that lead to a decimal value. sqrt(2)^2 == 2 ## [1] FALSE If two things you expect to be equal are not showing up as being TRUE and the values you are comparing are decimal values, you should use the near function instead. near(sqrt(2)^2, 2) ## [1] TRUE "],
["logical-operators.html", "4.4 Logical operators", " 4.4 Logical operators Logical operators allow you to build more complex boolean conditions. |: or &amp;: and Filter the month from the flights dataset where the month is 11 (November) or 12 (December) filter(flights, month == 11 | month == 12) ## # A tibble: 55,403 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 11 1 5 2359 6 352 ## 2 2013 11 1 35 2250 105 123 ## 3 2013 11 1 455 500 -5 641 ## 4 2013 11 1 539 545 -6 856 ## 5 2013 11 1 542 545 -3 831 ## 6 2013 11 1 549 600 -11 912 ## 7 2013 11 1 550 600 -10 705 ## 8 2013 11 1 554 600 -6 659 ## 9 2013 11 1 554 600 -6 826 ## 10 2013 11 1 554 600 -6 749 ## # ... with 55,393 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; The below code will not run like you would expect (even though this is how you would say it in your head) # filter(flights, month == 11 | 12) ## this is wrong and will not work like you expect filter(flights, month == 11 | 12) ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # ... with 336,766 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Instead of writing out each boolean statment separately using |, you can use the %in% operator filter(flights, month %in% c(11, 12)) ## # A tibble: 55,403 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 11 1 5 2359 6 352 ## 2 2013 11 1 35 2250 105 123 ## 3 2013 11 1 455 500 -5 641 ## 4 2013 11 1 539 545 -6 856 ## 5 2013 11 1 542 545 -3 831 ## 6 2013 11 1 549 600 -11 912 ## 7 2013 11 1 550 600 -10 705 ## 8 2013 11 1 554 600 -6 659 ## 9 2013 11 1 554 600 -6 826 ## 10 2013 11 1 554 600 -6 749 ## # ... with 55,393 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; WIth filter, you can also specify multiple condition (like an &amp;) filter(flights, arr_delay &lt;= 120, dep_delay &lt;= 12) ## # A tibble: 250,224 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # ... with 250,214 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; by default filter will also drop missing values. See the r4ds chapter for this. "],
["arrange.html", "4.5 Arrange", " 4.5 Arrange arrange(flights, year, month, day) ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # ... with 336,766 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; use desc to sort things in decending order arrange(flights, year, month, desc(day)) ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 31 1 2100 181 124 ## 2 2013 1 31 4 2359 5 455 ## 3 2013 1 31 7 2359 8 453 ## 4 2013 1 31 12 2250 82 132 ## 5 2013 1 31 26 2154 152 328 ## 6 2013 1 31 34 2159 155 135 ## 7 2013 1 31 37 2249 108 132 ## 8 2013 1 31 54 2250 124 152 ## 9 2013 1 31 453 500 -7 651 ## 10 2013 1 31 522 525 -3 820 ## # ... with 336,766 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; arrange(flights, year, desc(month), day) ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 12 1 13 2359 14 446 ## 2 2013 12 1 17 2359 18 443 ## 3 2013 12 1 453 500 -7 636 ## 4 2013 12 1 520 515 5 749 ## 5 2013 12 1 536 540 -4 845 ## 6 2013 12 1 540 550 -10 1005 ## 7 2013 12 1 541 545 -4 734 ## 8 2013 12 1 546 545 1 826 ## 9 2013 12 1 549 600 -11 648 ## 10 2013 12 1 550 600 -10 825 ## # ... with 336,766 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; "],
["select.html", "4.6 Select", " 4.6 Select select(flights, year, month, day) ## # A tibble: 336,776 x 3 ## year month day ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 ## 2 2013 1 1 ## 3 2013 1 1 ## 4 2013 1 1 ## 5 2013 1 1 ## 6 2013 1 1 ## 7 2013 1 1 ## 8 2013 1 1 ## 9 2013 1 1 ## 10 2013 1 1 ## # ... with 336,766 more rows select(flights, year:day, arr_delay) ## # A tibble: 336,776 x 4 ## year month day arr_delay ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 11 ## 2 2013 1 1 20 ## 3 2013 1 1 33 ## 4 2013 1 1 -18 ## 5 2013 1 1 -25 ## 6 2013 1 1 12 ## 7 2013 1 1 19 ## 8 2013 1 1 -14 ## 9 2013 1 1 -8 ## 10 2013 1 1 8 ## # ... with 336,766 more rows select(flights, -year) ## # A tibble: 336,776 x 18 ## month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 517 515 2 830 819 ## 2 1 1 533 529 4 850 830 ## 3 1 1 542 540 2 923 850 ## 4 1 1 544 545 -1 1004 1022 ## 5 1 1 554 600 -6 812 837 ## 6 1 1 554 558 -4 740 728 ## 7 1 1 555 600 -5 913 854 ## 8 1 1 557 600 -3 709 723 ## 9 1 1 557 600 -3 838 846 ## 10 1 1 558 600 -2 753 745 ## # ... with 336,766 more rows, and 11 more variables: arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, ## # time_hour &lt;dttm&gt; select(flights, -(year:day)) ## # A tibble: 336,776 x 16 ## dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 517 515 2 830 819 11 ## 2 533 529 4 850 830 20 ## 3 542 540 2 923 850 33 ## 4 544 545 -1 1004 1022 -18 ## 5 554 600 -6 812 837 -25 ## 6 554 558 -4 740 728 12 ## 7 555 600 -5 913 854 19 ## 8 557 600 -3 709 723 -14 ## 9 557 600 -3 838 846 -8 ## 10 558 600 -2 753 745 8 ## # ... with 336,766 more rows, and 10 more variables: carrier &lt;chr&gt;, ## # flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, ## # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Other functions you can use within select: starts_with ends_with contains matches num_range # for example to create x1, x2, x3 rename(flights, &quot;tail_num&quot; = tailnum, &#39;y&#39; = year) ## # A tibble: 336,776 x 19 ## y month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # ... with 336,766 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tail_num &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; select(flights, time_hour, air_time, everything()) ## # A tibble: 336,776 x 19 ## time_hour air_time year month day dep_time sched_dep_time ## &lt;dttm&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013-01-01 05:00:00 227 2013 1 1 517 515 ## 2 2013-01-01 05:00:00 227 2013 1 1 533 529 ## 3 2013-01-01 05:00:00 160 2013 1 1 542 540 ## 4 2013-01-01 05:00:00 183 2013 1 1 544 545 ## 5 2013-01-01 06:00:00 116 2013 1 1 554 600 ## 6 2013-01-01 05:00:00 150 2013 1 1 554 558 ## 7 2013-01-01 06:00:00 158 2013 1 1 555 600 ## 8 2013-01-01 06:00:00 53 2013 1 1 557 600 ## 9 2013-01-01 06:00:00 140 2013 1 1 557 600 ## 10 2013-01-01 06:00:00 138 2013 1 1 558 600 ## # ... with 336,766 more rows, and 12 more variables: dep_delay &lt;dbl&gt;, ## # arr_time &lt;int&gt;, sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, ## # flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt; "],
["mutate.html", "4.7 Mutate", " 4.7 Mutate (flights_sml &lt;- select(flights, year:day, ends_with(&#39;delay&#39;), distance, air_time)) ## # A tibble: 336,776 x 7 ## year month day dep_delay arr_delay distance air_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 2 11 1400 227 ## 2 2013 1 1 4 20 1416 227 ## 3 2013 1 1 2 33 1089 160 ## 4 2013 1 1 -1 -18 1576 183 ## 5 2013 1 1 -6 -25 762 116 ## 6 2013 1 1 -4 12 719 150 ## 7 2013 1 1 -5 19 1065 158 ## 8 2013 1 1 -3 -14 229 53 ## 9 2013 1 1 -3 -8 944 140 ## 10 2013 1 1 -2 8 733 138 ## # ... with 336,766 more rows mutate(flights_sml, gain = arr_delay - dep_delay, speed = distance / air_time * 60) ## # A tibble: 336,776 x 9 ## year month day dep_delay arr_delay distance air_time gain speed ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 2 11 1400 227 9 370. ## 2 2013 1 1 4 20 1416 227 16 374. ## 3 2013 1 1 2 33 1089 160 31 408. ## 4 2013 1 1 -1 -18 1576 183 -17 517. ## 5 2013 1 1 -6 -25 762 116 -19 394. ## 6 2013 1 1 -4 12 719 150 16 288. ## 7 2013 1 1 -5 19 1065 158 24 404. ## 8 2013 1 1 -3 -14 229 53 -11 259. ## 9 2013 1 1 -3 -8 944 140 -5 405. ## 10 2013 1 1 -2 8 733 138 10 319. ## # ... with 336,766 more rows mutate(flights_sml, gain = arr_delay - dep_delay, speed = distance / air_time * 60, hours = air_time / 60, gain_per_hour = gain / hours ) ## # A tibble: 336,776 x 11 ## year month day dep_delay arr_delay distance air_time gain speed ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 2 11 1400 227 9 370. ## 2 2013 1 1 4 20 1416 227 16 374. ## 3 2013 1 1 2 33 1089 160 31 408. ## 4 2013 1 1 -1 -18 1576 183 -17 517. ## 5 2013 1 1 -6 -25 762 116 -19 394. ## 6 2013 1 1 -4 12 719 150 16 288. ## 7 2013 1 1 -5 19 1065 158 24 404. ## 8 2013 1 1 -3 -14 229 53 -11 259. ## 9 2013 1 1 -3 -8 944 140 -5 405. ## 10 2013 1 1 -2 8 733 138 10 319. ## # ... with 336,766 more rows, and 2 more variables: hours &lt;dbl&gt;, ## # gain_per_hour &lt;dbl&gt; "],
["summarize-summarise.html", "4.8 Summarize (summarise)", " 4.8 Summarize (summarise) summarize(flights, delay = mean(dep_delay, na.rm = TRUE)) ## # A tibble: 1 x 1 ## delay ## &lt;dbl&gt; ## 1 12.6 "],
["groupby.html", "4.9 Groupby", " 4.9 Groupby by_day &lt;- group_by(flights, year, month, day) summarize(by_day, delay = mean(dep_delay, na.rm = TRUE)) ## # A tibble: 365 x 4 ## # Groups: year, month [?] ## year month day delay ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 11.5 ## 2 2013 1 2 13.9 ## 3 2013 1 3 11.0 ## 4 2013 1 4 8.95 ## 5 2013 1 5 5.73 ## 6 2013 1 6 7.15 ## 7 2013 1 7 5.42 ## 8 2013 1 8 2.55 ## 9 2013 1 9 2.28 ## 10 2013 1 10 2.84 ## # ... with 355 more rows Do a group by and perform multiple summarizations by_month &lt;- group_by(flights, year, month) by_month &lt;- summarize(by_month, delay = mean(dep_delay, na.rm = TRUE), delay_std = sd(dep_delay, na.rm = TRUE) ) by_month ## # A tibble: 12 x 4 ## # Groups: year [?] ## year month delay delay_std ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 10.0 36.4 ## 2 2013 2 10.8 36.3 ## 3 2013 3 13.2 40.1 ## 4 2013 4 13.9 43.0 ## 5 2013 5 13.0 39.4 ## 6 2013 6 20.8 51.5 ## 7 2013 7 21.7 51.6 ## 8 2013 8 12.6 37.7 ## 9 2013 9 6.72 35.6 ## 10 2013 10 6.24 29.7 ## 11 2013 11 5.44 27.6 ## 12 2013 12 16.6 41.9 The above code can be re-written using the pipe, %&gt;% # i&#39;m pretty sure this is easier to read and understand by_month &lt;- group_by(flights, year, month) %&gt;% summarize(delay = mean(dep_delay, na.rm = TRUE), delay_std = sd(dep_delay, na.rm = TRUE)) by_month ## # A tibble: 12 x 4 ## # Groups: year [?] ## year month delay delay_std ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 10.0 36.4 ## 2 2013 2 10.8 36.3 ## 3 2013 3 13.2 40.1 ## 4 2013 4 13.9 43.0 ## 5 2013 5 13.0 39.4 ## 6 2013 6 20.8 51.5 ## 7 2013 7 21.7 51.6 ## 8 2013 8 12.6 37.7 ## 9 2013 9 6.72 35.6 ## 10 2013 10 6.24 29.7 ## 11 2013 11 5.44 27.6 ## 12 2013 12 16.6 41.9 Otherwise you will have to create a temp variable, or write a nested expression summarize(group_by(flights, year, month), delay = mean(dep_delay, na.rm = TRUE), delay_std = sd(dep_delay, na.rm = TRUE)) ## # A tibble: 12 x 4 ## # Groups: year [?] ## year month delay delay_std ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 10.0 36.4 ## 2 2013 2 10.8 36.3 ## 3 2013 3 13.2 40.1 ## 4 2013 4 13.9 43.0 ## 5 2013 5 13.0 39.4 ## 6 2013 6 20.8 51.5 ## 7 2013 7 21.7 51.6 ## 8 2013 8 12.6 37.7 ## 9 2013 9 6.72 35.6 ## 10 2013 10 6.24 29.7 ## 11 2013 11 5.44 27.6 ## 12 2013 12 16.6 41.9 "],
["tidy-data.html", "Chapter 5 Tidy data", " Chapter 5 Tidy data Tidy data chapter is r4ds: http://r4ds.had.co.nz/tidy-data.html Hadley Wickham’s Tidy data paper http://vita.had.co.nz/papers/tidy-data.html DataCamp Courses: https://www.datacamp.com/courses/cleaning-data-in-r https://www.datacamp.com/courses/importing-cleaning-data-in-r-case-studies Resources http://tidyr.tidyverse.org/ "],
["tidyr.html", "5.1 tidyr", " 5.1 tidyr library(tidyr) table4a ## # A tibble: 3 x 3 ## country `1999` `2000` ## * &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 "],
["gather.html", "5.2 Gather", " 5.2 Gather table4a_tidy &lt;- gather(table4a, &#39;2000&#39;, &#39;1999&#39;, key = &quot;year&quot;, value = &#39;cases&#39;) library(ggplot2) ggplot(table4a_tidy) + geom_histogram(aes(x = cases, fill = country)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. (table4a_tidy &lt;- table4a %&gt;% gather(&#39;2000&#39;, &#39;1999&#39;, key = &quot;year&quot;, value = &#39;cases&#39;)) ## # A tibble: 6 x 3 ## country year cases ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 2000 2666 ## 2 Brazil 2000 80488 ## 3 China 2000 213766 ## 4 Afghanistan 1999 745 ## 5 Brazil 1999 37737 ## 6 China 1999 212258 library(magrittr) # this is what actuallly gives you the pipe ## ## Attaching package: &#39;magrittr&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract (table4b_tidy &lt;- table4b %&gt;% gather(&#39;1999&#39;:&#39;2000&#39;, key = &#39;year&#39;, value = &#39;population&#39;)) ## # A tibble: 6 x 3 ## country year population ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 19987071 ## 2 Brazil 1999 172006362 ## 3 China 1999 1272915272 ## 4 Afghanistan 2000 20595360 ## 5 Brazil 2000 174504898 ## 6 China 2000 1280428583 "],
["primer-to-joins.html", "5.3 Primer to joins", " 5.3 Primer to joins library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union # use double colon to specify which library you are getting a function from # base::union() table4a_tidy ## # A tibble: 6 x 3 ## country year cases ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 2000 2666 ## 2 Brazil 2000 80488 ## 3 China 2000 213766 ## 4 Afghanistan 1999 745 ## 5 Brazil 1999 37737 ## 6 China 1999 212258 table4b_tidy ## # A tibble: 6 x 3 ## country year population ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 19987071 ## 2 Brazil 1999 172006362 ## 3 China 1999 1272915272 ## 4 Afghanistan 2000 20595360 ## 5 Brazil 2000 174504898 ## 6 China 2000 1280428583 left_join(x = table4a_tidy, y = table4b_tidy) ## Joining, by = c(&quot;country&quot;, &quot;year&quot;) ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 2000 2666 20595360 ## 2 Brazil 2000 80488 174504898 ## 3 China 2000 213766 1280428583 ## 4 Afghanistan 1999 745 19987071 ## 5 Brazil 1999 37737 172006362 ## 6 China 1999 212258 1272915272 "],
["spread.html", "5.4 Spread", " 5.4 Spread table2 ## # A tibble: 12 x 4 ## country year type count ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 spread(table2, key = type, value = count) ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 "],
["separate.html", "5.5 Separate", " 5.5 Separate table3 ## # A tibble: 6 x 3 ## country year rate ## * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 ## 2 Afghanistan 2000 2666/20595360 ## 3 Brazil 1999 37737/172006362 ## 4 Brazil 2000 80488/174504898 ## 5 China 1999 212258/1272915272 ## 6 China 2000 213766/1280428583 table3 %&gt;% separate(rate, into = c(&#39;cases&#39;, &#39;population&#39;)) ## # A tibble: 6 x 4 ## country year cases population ## * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 table3 %&gt;% separate(rate, into = c(&#39;cases&#39;, &#39;population&#39;), sep = &#39;/&#39;) ## # A tibble: 6 x 4 ## country year cases population ## * &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 "],
["relational-data-in-r.html", "Chapter 6 Relational Data in R", " Chapter 6 Relational Data in R Relational data chapter in r4ds http://r4ds.had.co.nz/relational-data.html DataCamp Courses: https://www.datacamp.com/courses/joining-data-in-r-with-dplyr Resources: Rstudio dplyr Cheat Sheet dplyr join reference: https://dplyr.tidyverse.org/reference/join.html "],
["create-simple-datasets.html", "6.1 Create simple datasets", " 6.1 Create simple datasets library(tibble) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union x &lt;- tribble( ~key, ~val_x, 1, &quot;x1&quot;, 2, &quot;x2&quot;, 3, &quot;x3&quot; ) x ## # A tibble: 3 x 2 ## key val_x ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 x1 ## 2 2 x2 ## 3 3 x3 y &lt;- tribble( ~key, ~val_y, 1, &quot;y1&quot;, 2, &quot;y2&quot;, 4, &quot;y3&quot; ) y ## # A tibble: 3 x 2 ## key val_y ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 y1 ## 2 2 y2 ## 3 4 y3 "],
["one-to-one.html", "6.2 one-to-one", " 6.2 one-to-one When you perform a join, the function and code do not change. The output of the join will be determined by whether or not you have duplicate values in your keys, and whether or not both tables you are joining have duplicate values in the keys. In a one-to-one merge, there are no duplicate keys in both tables. x %&gt;% inner_join(y, by = &#39;key&#39;) ## # A tibble: 2 x 3 ## key val_x val_y ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 x1 y1 ## 2 2 x2 y2 x %&gt;% left_join(y, by = &#39;key&#39;) ## # A tibble: 3 x 3 ## key val_x val_y ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 3 x3 &lt;NA&gt; x %&gt;% right_join(y, by = &#39;key&#39;) ## # A tibble: 3 x 3 ## key val_x val_y ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 4 &lt;NA&gt; y3 x %&gt;% full_join(y, by = &#39;key&#39;) ## # A tibble: 4 x 3 ## key val_x val_y ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 3 x3 &lt;NA&gt; ## 4 4 &lt;NA&gt; y3 "],
["many-to-one-or-one-to-many.html", "6.3 many to one OR one to many", " 6.3 many to one OR one to many In a many-to-one merge, one of the tables have duplicate keys (x &lt;- tribble( ~key, ~val_x, 1, &quot;x1&quot;, 2, &quot;x2&quot;, 3, &quot;x3&quot;, 1, &quot;x4&quot;, 1, &quot;x5&quot;, 1, &quot;x6&quot; )) ## # A tibble: 6 x 2 ## key val_x ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 x1 ## 2 2 x2 ## 3 3 x3 ## 4 1 x4 ## 5 1 x5 ## 6 1 x6 (y &lt;- tribble( ~key, ~val_y, 1, &quot;y1&quot;, 2, &quot;y2&quot;, 4, &quot;y3&quot; )) ## # A tibble: 3 x 2 ## key val_y ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 y1 ## 2 2 y2 ## 3 4 y3 x %&gt;% left_join(y, by = &#39;key&#39;) ## # A tibble: 6 x 3 ## key val_x val_y ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 x1 y1 ## 2 2 x2 y2 ## 3 3 x3 &lt;NA&gt; ## 4 1 x4 y1 ## 5 1 x5 y1 ## 6 1 x6 y1 "],
["many-to-many.html", "6.4 many to many", " 6.4 many to many In a many to many join, both tables will have duplicate keys in them. You end up with what’s known as a cartesian product of the keys. (x &lt;- tribble( ~key, ~val_x, 1, &quot;x1&quot;, 2, &quot;x2&quot;, 3, &quot;x3&quot;, 1, &quot;x4&quot;, 1, &quot;x5&quot;, 1, &quot;x6&quot; )) ## # A tibble: 6 x 2 ## key val_x ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 x1 ## 2 2 x2 ## 3 3 x3 ## 4 1 x4 ## 5 1 x5 ## 6 1 x6 (y &lt;- tribble( ~key, ~val_y, 1, &quot;y1&quot;, 2, &quot;y2&quot;, 4, &quot;y3&quot;, 1, &quot;y4&quot;, 1, &quot;y5&quot; )) ## # A tibble: 5 x 2 ## key val_y ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 y1 ## 2 2 y2 ## 3 4 y3 ## 4 1 y4 ## 5 1 y5 A many to many join creates a cartesian product x %&gt;% left_join(y, by = &#39;key&#39;) ## # A tibble: 14 x 3 ## key val_x val_y ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 x1 y1 ## 2 1 x1 y4 ## 3 1 x1 y5 ## 4 2 x2 y2 ## 5 3 x3 &lt;NA&gt; ## 6 1 x4 y1 ## 7 1 x4 y4 ## 8 1 x4 y5 ## 9 1 x5 y1 ## 10 1 x5 y4 ## 11 1 x5 y5 ## 12 1 x6 y1 ## 13 1 x6 y4 ## 14 1 x6 y5 "],
["multiple-keys.html", "6.5 Multiple keys", " 6.5 Multiple keys The below code won’t work, but it’s to show you that you can join by multiple keys x %&gt;% left_join(y, by = c(&#39;key&#39;, &#39;col2&#39;, &#39;col3&#39;)) "],
["different-column-names.html", "6.6 Different column names", " 6.6 Different column names You can also join tables when columns do not share the same name x %&gt;% left_join(y, by = c(&#39;key&#39; = &#39;key2&#39;, &#39;col2&#39; = &#39;other&#39;, &#39;col3&#39;)) "],
["factors.html", "Chapter 7 Factors", " Chapter 7 Factors Factors chapter in r4ds http://r4ds.had.co.nz/factors.html DataCamp Courses Resources http://forcats.tidyverse.org/ https://github.com/tidyverse/forcats "],
["forcats.html", "7.1 forcats", " 7.1 forcats library(forcats) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(ggplot2) x1 &lt;- c(&quot;M&quot;, &quot;T&quot;, &quot;W&quot;, &quot;R&quot;, &quot;F&quot;, &quot;S&quot;, &quot;U&quot;) x2 &lt;- c(&quot;Sa&quot;, &quot;Su&quot;) "],
["sorting.html", "7.2 Sorting", " 7.2 Sorting sort(x1) ## [1] &quot;F&quot; &quot;M&quot; &quot;R&quot; &quot;S&quot; &quot;T&quot; &quot;U&quot; &quot;W&quot; proper_order &lt;- x1 cat1 &lt;- factor(x1, levels = proper_order) cat1 ## [1] M T W R F S U ## Levels: M T W R F S U sort(cat1) ## [1] M T W R F S U ## Levels: M T W R F S U "],
["counting.html", "7.3 Counting", " 7.3 Counting head(gss_cat) ## # A tibble: 6 x 9 ## year marital age race rincome partyid relig denom tvhours ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 2000 Never married 26 White $8000 t… Ind,near… Prote… South… 12 ## 2 2000 Divorced 48 White $8000 t… Not str … Prote… Bapti… NA ## 3 2000 Widowed 67 White Not app… Independ… Prote… No de… 2 ## 4 2000 Never married 39 White Not app… Ind,near… Ortho… Not a… 4 ## 5 2000 Divorced 25 White Not app… Not str … None Not a… 1 ## 6 2000 Married 25 White $20000 … Strong d… Prote… South… NA dplyr::count(gss_cat, marital) ## # A tibble: 6 x 2 ## marital n ## &lt;fct&gt; &lt;int&gt; ## 1 No answer 17 ## 2 Never married 5416 ## 3 Separated 743 ## 4 Divorced 3383 ## 5 Widowed 1807 ## 6 Married 10117 gss_cat %&gt;% dplyr::count(marital) ## # A tibble: 6 x 2 ## marital n ## &lt;fct&gt; &lt;int&gt; ## 1 No answer 17 ## 2 Never married 5416 ## 3 Separated 743 ## 4 Divorced 3383 ## 5 Widowed 1807 ## 6 Married 10117 "],
["re-ording-factors.html", "7.4 Re-ording factors", " 7.4 Re-ording factors library(ggplot2) library(dplyr) relig_summary &lt;- gss_cat %&gt;% dplyr::group_by(relig) %&gt;% dplyr::summarize( tvhours = mean(tvhours, na.rm = TRUE), age = mean(age, na.rm = TRUE), n = n() ) relig_summary ## # A tibble: 15 x 4 ## relig tvhours age n ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 No answer 2.72 49.5 93 ## 2 Don&#39;t know 4.62 35.9 15 ## 3 Inter-nondenominational 2.87 40.0 109 ## 4 Native american 3.46 38.9 23 ## 5 Christian 2.79 40.1 689 ## 6 Orthodox-christian 2.42 50.4 95 ## 7 Moslem/islam 2.44 37.6 104 ## 8 Other eastern 1.67 45.9 32 ## 9 Hinduism 1.89 37.7 71 ## 10 Buddhism 2.38 44.7 147 ## 11 Other 2.73 41.0 224 ## 12 None 2.71 41.2 3523 ## 13 Jewish 2.52 52.4 388 ## 14 Catholic 2.96 46.9 5124 ## 15 Protestant 3.15 49.9 10846 ggplot(relig_summary, aes(x = tvhours, y = relig)) + geom_point() ggplot(relig_summary, aes(x = tvhours, y = fct_reorder(relig, tvhours))) + geom_point() "],
["modifying-factor-values.html", "7.5 Modifying factor values", " 7.5 Modifying factor values gss_cat %&gt;% count(partyid) ## # A tibble: 10 x 2 ## partyid n ## &lt;fct&gt; &lt;int&gt; ## 1 No answer 154 ## 2 Don&#39;t know 1 ## 3 Other party 393 ## 4 Strong republican 2314 ## 5 Not str republican 3032 ## 6 Ind,near rep 1791 ## 7 Independent 4119 ## 8 Ind,near dem 2499 ## 9 Not str democrat 3690 ## 10 Strong democrat 3490 # same as below #forcats::fct_recode(gss_cat$partyid) forcats::fct_recode(gss_cat$partyid, &quot;Rep, Strong&quot; = &quot;Strong republican&quot;, &quot;Dem, Strong&quot; = &quot;Strong democrat&quot;) %&gt;% head(30) ## [1] Ind,near rep Not str republican Independent ## [4] Ind,near rep Not str democrat Dem, Strong ## [7] Not str republican Ind,near dem Not str democrat ## [10] Rep, Strong Not str democrat Ind,near rep ## [13] Dem, Strong Rep, Strong Ind,near dem ## [16] Dem, Strong Rep, Strong Independent ## [19] Not str democrat Independent Ind,near dem ## [22] Rep, Strong Independent Ind,near rep ## [25] Not str democrat Dem, Strong Not str democrat ## [28] Rep, Strong Dem, Strong Independent ## 10 Levels: No answer Don&#39;t know Other party ... Dem, Strong gss_cat_recoded &lt;- gss_cat %&gt;% mutate(party_id_recode = fct_recode( partyid, &quot;Rep, Strong&quot; = &quot;Strong republican&quot;, &quot;Dem, Strong&quot; = &quot;Strong democrat&quot;)) 7.5.1 Double check your work https://gist.github.com/jennybc/04b71bfaaf0f88d9d2eb # do a cross tab in R table(gss_cat_recoded$partyid, gss_cat_recoded$party_id_recode, useNA = &#39;always&#39;) ## ## No answer Don&#39;t know Other party Rep, Strong ## No answer 154 0 0 0 ## Don&#39;t know 0 1 0 0 ## Other party 0 0 393 0 ## Strong republican 0 0 0 2314 ## Not str republican 0 0 0 0 ## Ind,near rep 0 0 0 0 ## Independent 0 0 0 0 ## Ind,near dem 0 0 0 0 ## Not str democrat 0 0 0 0 ## Strong democrat 0 0 0 0 ## &lt;NA&gt; 0 0 0 0 ## ## Not str republican Ind,near rep Independent ## No answer 0 0 0 ## Don&#39;t know 0 0 0 ## Other party 0 0 0 ## Strong republican 0 0 0 ## Not str republican 3032 0 0 ## Ind,near rep 0 1791 0 ## Independent 0 0 4119 ## Ind,near dem 0 0 0 ## Not str democrat 0 0 0 ## Strong democrat 0 0 0 ## &lt;NA&gt; 0 0 0 ## ## Ind,near dem Not str democrat Dem, Strong &lt;NA&gt; ## No answer 0 0 0 0 ## Don&#39;t know 0 0 0 0 ## Other party 0 0 0 0 ## Strong republican 0 0 0 0 ## Not str republican 0 0 0 0 ## Ind,near rep 0 0 0 0 ## Independent 0 0 0 0 ## Ind,near dem 2499 0 0 0 ## Not str democrat 0 3690 0 0 ## Strong democrat 0 0 3490 0 ## &lt;NA&gt; 0 0 0 0 "],
["dates-and-times.html", "Chapter 8 Dates and Times", " Chapter 8 Dates and Times Resources http://lubridate.tidyverse.org/ https://github.com/tidyverse/lubridate Cheat Sheets https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf "],
["lubridate.html", "8.1 lubridate", " 8.1 lubridate library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following object is masked from &#39;package:base&#39;: ## ## date ymd(&#39;2018-06-06&#39;) ## [1] &quot;2018-06-06&quot; dt_str &lt;- &#39;2018-06-06&#39; class(dt_str) ## [1] &quot;character&quot; dt_dt &lt;- ymd(dt_str) class(dt_dt) ## [1] &quot;Date&quot; mdy(&#39;June 6, 2018&#39;) ## [1] &quot;2018-06-06&quot; dmy(&#39;06-06-2018&#39;) ## [1] &quot;2018-06-06&quot; ymd(20190606) ## [1] &quot;2019-06-06&quot; ymd_hms(&#39;2018-06-06 10:33:55&#39;, tz = &#39;EDT&#39;) ## [1] &quot;2018-06-06 14:33:55 EDT&quot; "],
["making-datetimes-from-data.html", "8.2 Making datetimes from data", " 8.2 Making datetimes from data library(nycflights13) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:lubridate&#39;: ## ## intersect, setdiff, union ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union flights ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # ... with 336,766 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; flight_times &lt;- flights %&gt;% select(year, month, day, hour, minute) flight_times ## # A tibble: 336,776 x 5 ## year month day hour minute ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2013 1 1 5 15 ## 2 2013 1 1 5 29 ## 3 2013 1 1 5 40 ## 4 2013 1 1 5 45 ## 5 2013 1 1 6 0 ## 6 2013 1 1 5 58 ## 7 2013 1 1 6 0 ## 8 2013 1 1 6 0 ## 9 2013 1 1 6 0 ## 10 2013 1 1 6 0 ## # ... with 336,766 more rows flight_times %&gt;% mutate(dep_dt = make_datetime(year, month, day, hour, minute)) ## # A tibble: 336,776 x 6 ## year month day hour minute dep_dt ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; ## 1 2013 1 1 5 15 2013-01-01 05:15:00 ## 2 2013 1 1 5 29 2013-01-01 05:29:00 ## 3 2013 1 1 5 40 2013-01-01 05:40:00 ## 4 2013 1 1 5 45 2013-01-01 05:45:00 ## 5 2013 1 1 6 0 2013-01-01 06:00:00 ## 6 2013 1 1 5 58 2013-01-01 05:58:00 ## 7 2013 1 1 6 0 2013-01-01 06:00:00 ## 8 2013 1 1 6 0 2013-01-01 06:00:00 ## 9 2013 1 1 6 0 2013-01-01 06:00:00 ## 10 2013 1 1 6 0 2013-01-01 06:00:00 ## # ... with 336,766 more rows flights %&gt;% select(year, month, day, hour, minute) %&gt;% mutate(dep_dt = make_datetime(year, month, day, hour, minute)) ## # A tibble: 336,776 x 6 ## year month day hour minute dep_dt ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; ## 1 2013 1 1 5 15 2013-01-01 05:15:00 ## 2 2013 1 1 5 29 2013-01-01 05:29:00 ## 3 2013 1 1 5 40 2013-01-01 05:40:00 ## 4 2013 1 1 5 45 2013-01-01 05:45:00 ## 5 2013 1 1 6 0 2013-01-01 06:00:00 ## 6 2013 1 1 5 58 2013-01-01 05:58:00 ## 7 2013 1 1 6 0 2013-01-01 06:00:00 ## 8 2013 1 1 6 0 2013-01-01 06:00:00 ## 9 2013 1 1 6 0 2013-01-01 06:00:00 ## 10 2013 1 1 6 0 2013-01-01 06:00:00 ## # ... with 336,766 more rows "],
["non-standard-date-formatting.html", "8.3 Non-standard date formatting", " 8.3 Non-standard date formatting # an example of a non-standard date format &#39;WED 06-JUNE-18 10:47:30 AM&#39; ## [1] &quot;WED 06-JUNE-18 10:47:30 AM&quot; If you look at the as_datetime function, under the ‘format’ section there is a link to the strptime documentation ?lubridate::as_datetime If you look at the strptime documentation, you will get a nice table of codes you can use to create your own datetime pattern ?strptime You can now use these variables to create a pattern for your custom datetime string curr_time &lt;- lubridate::as_datetime(&#39;WED 06-JUNE-18 10:47:30 AM&#39;, format = &#39;%a %d-%B-%y %I:%M:%S %p&#39;, tz = &quot;EST&quot;) curr_time ## [1] &quot;2018-06-06 10:47:30 EST&quot; "],
["strptime-format-variables.html", "8.4 strptime format variables", " 8.4 strptime format variables The table of values in strptime have been reproduced below %a Abbreviated weekday name in the current locale on this platform. (Also matches full name on input: in some locales there are no abbreviations of names.) %A Full weekday name in the current locale. (Also matches abbreviated name on input.) %b Abbreviated month name in the current locale on this platform. (Also matches full name on input: in some locales there are no abbreviations of names.) %B Full month name in the current locale. (Also matches abbreviated name on input.) %c Date and time. Locale-specific on output, &quot;%a %b %e %H:%M:%S %Y&quot; on input. %C Century (00–99): the integer part of the year divided by 100. %d Day of the month as decimal number (01–31). %D Date format such as %m/%d/%y: the C99 standard says it should be that exact format (but not all OSes comply). %e Day of the month as decimal number (1–31), with a leading space for a single-digit number. %F Equivalent to %Y-%m-%d (the ISO 8601 date format). %g The last two digits of the week-based year (see %V). (Accepted but ignored on input.) %G The week-based year (see %V) as a decimal number. (Accepted but ignored on input.) %h Equivalent to %b. %H Hours as decimal number (00–23). As a special exception strings such as 24:00:00 are accepted for input, since ISO 8601 allows these. %I Hours as decimal number (01–12). %j Day of year as decimal number (001–366). %m Month as decimal number (01–12). %M Minute as decimal number (00–59). %n Newline on output, arbitrary whitespace on input. %p AM/PM indicator in the locale. Used in conjunction with %I and not with %H. An empty string in some locales (for example on some OSes, non-English European locales including Russia). The behaviour is undefined if used for input in such a locale. Some platforms accept %P for output, which uses a lower-case version (%p may also use lower case): others will output P. %r For output, the 12-hour clock time (using the locale&#39;s AM or PM): only defined in some locales, and on some OSes misleading in locales which do not define an AM/PM indicator. For input, equivalent to %I:%M:%S %p. %R Equivalent to %H:%M. %S Second as integer (00–61), allowing for up to two leap-seconds (but POSIX-compliant implementations will ignore leap seconds). %t Tab on output, arbitrary whitespace on input. %T Equivalent to %H:%M:%S. %u Weekday as a decimal number (1–7, Monday is 1). %U Week of the year as decimal number (00–53) using Sunday as the first day 1 of the week (and typically with the first Sunday of the year as day 1 of week 1). The US convention. %V Week of the year as decimal number (01–53) as defined in ISO 8601. If the week (starting on Monday) containing 1 January has four or more days in the new year, then it is considered week 1. Otherwise, it is the last week of the previous year, and the next week is week 1. (Accepted but ignored on input.) %w Weekday as decimal number (0–6, Sunday is 0). %W Week of the year as decimal number (00–53) using Monday as the first day of week (and typically with the first Monday of the year as day 1 of week 1). The UK convention. %x Date. Locale-specific on output, &quot;%y/%m/%d&quot; on input. %X Time. Locale-specific on output, &quot;%H:%M:%S&quot; on input. %y Year without century (00–99). On input, values 00 to 68 are prefixed by 20 and 69 to 99 by 19 – that is the behaviour specified by the 2004 and 2008 POSIX standards, but they do also say ‘it is expected that in a future version the default century inferred from a 2-digit year will change’. %Y Year with century. Note that whereas there was no zero in the original Gregorian calendar, ISO 8601:2004 defines it to be valid (interpreted as 1BC): see https://en.wikipedia.org/wiki/0_(year). Note that the standards also say that years before 1582 in its calendar should only be used with agreement of the parties involved. For input, only years 0:9999 are accepted. %z Signed offset in hours and minutes from UTC, so -0800 is 8 hours behind UTC. Values up to +1400 are accepted. (Standard only for output.) %Z (Output only.) Time zone abbreviation as a character string (empty if not available). This may not be reliable when a time zone has changed abbreviations over the years. "],
["datetime-arithmetic.html", "8.5 Datetime arithmetic", " 8.5 Datetime arithmetic Once you have a datetime object, you can then begin to do calculations and arithmetic on them. now() - curr_time ## Time difference of 6.578948 hours "],
["git-1.html", "Chapter 9 Git", " Chapter 9 Git Software-Carpentry Git Lesson DataCamp Courses: Introduction to Git for Data Science Working with the RStudio IDE (Part 2) – Chapter 2: Version Control Quick References: Software-Carpentry Reference Git Cheat Sheet (by Github) Jenny Bryan’s “Happy Git and GitHub for the useR” Git interaction from NDP Software Learn Git Branching Git and the “final” version problem If these comics bring back haunting memories, then version control is for you! Technically, renaming copies of files is a form of version control. It allows you to go back to a specific state of a file. As the two comics point out, this usually ends up in a cacophony of files with similar names. What about files and programs that know how to track changes already. I’m mainly thinking about Word documents. "],
["git-on-your-own.html", "9.1 Git on your own", " 9.1 Git on your own Figure 9.1: Diagram of Git commands and how they relate to one another. How not to write commit messages: how #not to write #git #commit messages -.-'' pic.twitter.com/5TdiZ1yi5S — Dⓐniel Chen ((???)) April 16, 2015 "],
["git-with-branches.html", "9.2 Git with branches", " 9.2 Git with branches Figure 9.2: Review of Git Figure 9.3: What branching looks like in the Git world "],
["collaborating-with-git.html", "9.3 Collaborating with Git", " 9.3 Collaborating with Git Figure .: The ‘forking’ model of Git workflows Figure .: Git with branches "],
["protecting-branches.html", "9.4 Protecting branches", " 9.4 Protecting branches https://docs.gitlab.com/ee/user/project/protected_branches.html In a repository go to settings &gt; repository &gt; protected branches set “allowed to merge”: masters “allowed to push”: no one If you accidently did work on master: create a branch where you are now: git branch BRANCH_NAME reset master to where you were: git reset --hard COMMIT_HASH_FOR_MASTER make sure you do this on the master branch go to your branch: git checkout BRANCH_NAME push your branch: git push origin BRANCH_NAME create and merge the pull/merge request "],
["build-details.html", "Build Details", " Build Details Sys.time() ## [1] &quot;2018-06-06 18:22:15 EDT&quot; sessionInfo() ## R version 3.5.0 (2018-04-23) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Arch Linux ## ## Matrix products: default ## BLAS: /usr/lib/libblasp-r0.3.0.dev.so ## LAPACK: /usr/lib/liblapack.so.3.8.0 ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_3.5.0 backports_1.1.2 bookdown_0.7 magrittr_1.5 ## [5] rprojroot_1.3-2 tools_3.5.0 htmltools_0.3.6 yaml_2.1.19 ## [9] Rcpp_0.12.17 stringi_1.2.2 rmarkdown_1.9 knitr_1.20 ## [13] xfun_0.1 stringr_1.3.1 digest_0.6.15 evaluate_0.10.1 devtools::session_info() ## Session info ------------------------------------------------------------- ## setting value ## version R version 3.5.0 (2018-04-23) ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## tz America/New_York ## date 2018-06-06 ## Packages ----------------------------------------------------------------- ## package * version date source ## backports 1.1.2 2017-12-13 CRAN (R 3.5.0) ## base * 3.5.0 2018-04-23 local ## bookdown 0.7 2018-02-18 CRAN (R 3.5.0) ## compiler 3.5.0 2018-04-23 local ## datasets * 3.5.0 2018-04-23 local ## devtools 1.13.5 2018-02-18 CRAN (R 3.5.0) ## digest 0.6.15 2018-01-28 CRAN (R 3.5.0) ## evaluate 0.10.1 2017-06-24 CRAN (R 3.5.0) ## graphics * 3.5.0 2018-04-23 local ## grDevices * 3.5.0 2018-04-23 local ## htmltools 0.3.6 2017-04-28 CRAN (R 3.5.0) ## knitr 1.20 2018-02-20 CRAN (R 3.5.0) ## magrittr 1.5 2014-11-22 CRAN (R 3.5.0) ## memoise 1.1.0 2017-04-21 CRAN (R 3.5.0) ## methods * 3.5.0 2018-04-23 local ## Rcpp 0.12.17 2018-05-18 CRAN (R 3.5.0) ## rmarkdown 1.9 2018-03-01 CRAN (R 3.5.0) ## rprojroot 1.3-2 2018-01-03 CRAN (R 3.5.0) ## stats * 3.5.0 2018-04-23 local ## stringi 1.2.2 2018-05-02 CRAN (R 3.5.0) ## stringr 1.3.1 2018-05-10 CRAN (R 3.5.0) ## tools 3.5.0 2018-04-23 local ## utils * 3.5.0 2018-04-23 local ## withr 2.1.2 2018-03-15 CRAN (R 3.5.0) ## xfun 0.1 2018-01-22 CRAN (R 3.5.0) ## yaml 2.1.19 2018-05-01 CRAN (R 3.5.0) The all data revolution! "],
["references.html", "References", " References "]
]
